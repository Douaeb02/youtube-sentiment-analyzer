"""
Script pour visualiser les r√©sultats du mod√®le
"""
import json
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from pathlib import Path

def load_training_history():
    """Charge l'historique d'entra√Ænement"""
    history_file = Path('models/training_history.json')
    
    if not history_file.exists():
        print("‚ùå Fichier training_history.json introuvable!")
        return None
    
    with open(history_file, 'r') as f:
        history = json.load(f)
    
    return history

def plot_confusion_matrix(history):
    """Affiche la matrice de confusion"""
    if 'metrics' not in history or 'confusion_matrix' not in history['metrics']:
        print("‚ö†Ô∏è  Matrice de confusion non disponible")
        return
    
    cm = np.array(history['metrics']['confusion_matrix'])
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(
        cm, 
        annot=True, 
        fmt='d', 
        cmap='Blues',
        xticklabels=['N√©gatif', 'Neutre', 'Positif'],
        yticklabels=['N√©gatif', 'Neutre', 'Positif'],
        cbar_kws={'label': 'Nombre de pr√©dictions'}
    )
    
    plt.title('Matrice de Confusion', fontsize=16, fontweight='bold')
    plt.ylabel('Vraie Classe', fontsize=12)
    plt.xlabel('Classe Pr√©dite', fontsize=12)
    plt.tight_layout()
    
    # Sauvegarder
    output_path = Path('models/confusion_matrix.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"‚úÖ Matrice de confusion sauvegard√©e: {output_path}")
    
    plt.show()

def plot_metrics_by_class(history):
    """Affiche les m√©triques par classe"""
    if 'metrics' not in history:
        print("‚ö†Ô∏è  M√©triques non disponibles")
        return
    
    metrics = history['metrics']
    classes = ['N√©gatif', 'Neutre', 'Positif']
    
    precision = metrics.get('precision', [])
    recall = metrics.get('recall', [])
    f1_score = metrics.get('f1_score', [])
    
    x = np.arange(len(classes))
    width = 0.25
    
    fig, ax = plt.subplots(figsize=(12, 6))
    
    bars1 = ax.bar(x - width, precision, width, label='Precision', color='#3498db')
    bars2 = ax.bar(x, recall, width, label='Recall', color='#2ecc71')
    bars3 = ax.bar(x + width, f1_score, width, label='F1-Score', color='#e74c3c')
    
    ax.set_xlabel('Classes', fontsize=12)
    ax.set_ylabel('Score', fontsize=12)
    ax.set_title('M√©triques par Classe', fontsize=16, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(classes)
    ax.legend()
    ax.set_ylim([0, 1.1])
    ax.grid(axis='y', alpha=0.3)
    
    # Ajouter les valeurs sur les barres
    def add_values(bars):
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.3f}',
                   ha='center', va='bottom', fontsize=9)
    
    add_values(bars1)
    add_values(bars2)
    add_values(bars3)
    
    plt.tight_layout()
    
    # Sauvegarder
    output_path = Path('models/metrics_by_class.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"‚úÖ M√©triques par classe sauvegard√©es: {output_path}")
    
    plt.show()

def plot_performance_summary(history):
    """Affiche un r√©sum√© des performances"""
    if 'metrics' not in history:
        print("‚ö†Ô∏è  M√©triques non disponibles")
        return
    
    metrics = history['metrics']
    
    # Pr√©parer les donn√©es
    data = {
        'Train Accuracy': metrics.get('train_accuracy', 0),
        'Test Accuracy': metrics.get('test_accuracy', 0),
        'Avg F1-Score': metrics.get('avg_f1', 0)
    }
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    
    # Graphique 1: Accuracy
    colors = ['#3498db', '#2ecc71', '#e74c3c']
    bars = ax1.bar(data.keys(), data.values(), color=colors, alpha=0.7)
    ax1.set_ylabel('Score', fontsize=12)
    ax1.set_title('Performance Globale du Mod√®le', fontsize=14, fontweight='bold')
    ax1.set_ylim([0, 1.1])
    ax1.grid(axis='y', alpha=0.3)
    
    # Ajouter les valeurs
    for bar in bars:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.2%}',
                ha='center', va='bottom', fontsize=11, fontweight='bold')
    
    # Graphique 2: Temps d'inf√©rence
    inference_time = metrics.get('inference_time_ms', 0)
    batch_50_time = inference_time * 50
    
    times = [inference_time, batch_50_time]
    labels = ['1 commentaire', '50 commentaires']
    
    bars2 = ax2.bar(labels, times, color=['#9b59b6', '#f39c12'], alpha=0.7)
    ax2.set_ylabel('Temps (ms)', fontsize=12)
    ax2.set_title('Performance d\'Inf√©rence', fontsize=14, fontweight='bold')
    ax2.grid(axis='y', alpha=0.3)
    
    # Ajouter les valeurs
    for bar in bars2:
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.2f}ms',
                ha='center', va='bottom', fontsize=11, fontweight='bold')
    
    # Ligne de r√©f√©rence √† 100ms pour batch de 50
    ax2.axhline(y=100, color='r', linestyle='--', alpha=0.5, label='Objectif (100ms)')
    ax2.legend()
    
    plt.tight_layout()
    
    # Sauvegarder
    output_path = Path('models/performance_summary.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"‚úÖ R√©sum√© des performances sauvegard√©: {output_path}")
    
    plt.show()

def generate_report():
    """G√©n√®re un rapport texte"""
    history = load_training_history()
    
    if not history:
        return
    
    report_path = Path('models/performance_report.txt')
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("="*70 + "\n")
        f.write("RAPPORT DE PERFORMANCE DU MOD√àLE DE SENTIMENT\n")
        f.write("="*70 + "\n\n")
        
        # Informations g√©n√©rales
        f.write(f"Type de mod√®le: {history.get('model_type', 'N/A')}\n")
        f.write(f"Temps d'entra√Ænement: {history.get('train_time', 0):.2f}s\n\n")
        
        # M√©triques
        if 'metrics' in history:
            metrics = history['metrics']
            
            f.write("M√âTRIQUES DE PERFORMANCE\n")
            f.write("-"*70 + "\n")
            f.write(f"Train Accuracy:  {metrics.get('train_accuracy', 0):.4f} ({metrics.get('train_accuracy', 0)*100:.2f}%)\n")
            f.write(f"Test Accuracy:   {metrics.get('test_accuracy', 0):.4f} ({metrics.get('test_accuracy', 0)*100:.2f}%)\n")
            f.write(f"Avg F1-Score:    {metrics.get('avg_f1', 0):.4f}\n\n")
            
            f.write("M√âTRIQUES PAR CLASSE\n")
            f.write("-"*70 + "\n")
            classes = ['N√©gatif (-1)', 'Neutre (0)', 'Positif (1)']
            
            precision = metrics.get('precision', [])
            recall = metrics.get('recall', [])
            f1 = metrics.get('f1_score', [])
            
            f.write(f"{'Classe':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\n")
            f.write("-"*70 + "\n")
            
            for i, class_name in enumerate(classes):
                f.write(f"{class_name:<15} {precision[i]:>10.4f}  {recall[i]:>10.4f}  {f1[i]:>10.4f}\n")
            
            f.write("\n")
            f.write("PERFORMANCE D'INF√âRENCE\n")
            f.write("-"*70 + "\n")
            inf_time = metrics.get('inference_time_ms', 0)
            f.write(f"Temps par commentaire: {inf_time:.2f}ms\n")
            f.write(f"Temps pour 50 commentaires: {inf_time*50:.2f}ms\n")
            
            status = "‚úÖ ATTEINT" if inf_time*50 < 100 else "‚ùå NON ATTEINT"
            f.write(f"Objectif (<100ms): {status}\n")
    
    print(f"‚úÖ Rapport textuel sauvegard√©: {report_path}")

def main():
    """Fonction principale"""
    print("\n" + "üìä "*25)
    print("VISUALISATION DES R√âSULTATS")
    print("üìä "*25 + "\n")
    
    # Charger l'historique
    history = load_training_history()
    
    if not history:
        print("‚ùå Impossible de charger l'historique d'entra√Ænement")
        return
    
    print("‚úÖ Historique charg√©\n")
    
    # G√©n√©rer les visualisations
    print("üìà G√©n√©ration des graphiques...\n")
    
    plot_confusion_matrix(history)
    plot_metrics_by_class(history)
    plot_performance_summary(history)
    
    # G√©n√©rer le rapport
    print("\nüìù G√©n√©ration du rapport...\n")
    generate_report()
    
    print("\n" + "="*70)
    print("‚úÖ Toutes les visualisations ont √©t√© g√©n√©r√©es!")
    print("="*70)
    print("\nüìÅ Fichiers cr√©√©s dans le dossier models/:")
    print("  ‚Ä¢ confusion_matrix.png")
    print("  ‚Ä¢ metrics_by_class.png")
    print("  ‚Ä¢ performance_summary.png")
    print("  ‚Ä¢ performance_report.txt")

if __name__ == "__main__":
    # Configuration matplotlib
    plt.style.use('seaborn-v0_8-darkgrid')
    sns.set_palette("husl")
    
    main()