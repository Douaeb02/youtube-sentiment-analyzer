"""
API FastAPI pour l'analyse de sentiment YouTube
"""
from fastapi import FastAPI, HTTPException, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import time
import logging
from typing import List

from src.api.models import (
    CommentBatch,
    SentimentPrediction,
    BatchPredictionResponse,
    HealthResponse,
    ErrorResponse
)
from src.api.prediction_service import get_prediction_service

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Cr√©er l'application FastAPI
app = FastAPI(
    title="YouTube Sentiment Analyzer API",
    description="API REST pour l'analyse de sentiment des commentaires YouTube",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# Configuration CORS pour autoriser les requ√™tes depuis l'extension Chrome
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # En production, sp√©cifiez les origines autoris√©es
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Charger le service de pr√©diction au d√©marrage
@app.on_event("startup")
async def startup_event():
    """√âv√©nement ex√©cut√© au d√©marrage de l'API"""
    logger.info("üöÄ D√©marrage de l'API YouTube Sentiment Analyzer")
    try:
        service = get_prediction_service()
        if service.is_loaded():
            logger.info("‚úÖ Service de pr√©diction initialis√© avec succ√®s")
        else:
            logger.error("‚ùå √âchec de l'initialisation du service")
    except Exception as e:
        logger.error(f"‚ùå Erreur au d√©marrage: {e}")

@app.on_event("shutdown")
async def shutdown_event():
    """√âv√©nement ex√©cut√© √† l'arr√™t de l'API"""
    logger.info("üëã Arr√™t de l'API YouTube Sentiment Analyzer")

# ============================================================================
# ENDPOINTS
# ============================================================================

@app.get("/", tags=["Info"])
async def root():
    """Endpoint racine avec informations sur l'API"""
    return {
        "message": "YouTube Sentiment Analyzer API",
        "version": "1.0.0",
        "status": "running",
        "endpoints": {
            "health": "/health",
            "predict_batch": "/predict_batch",
            "docs": "/docs"
        }
    }

@app.get("/health", response_model=HealthResponse, tags=["Health"])
async def health_check():
    """
    Endpoint de sant√© pour v√©rifier l'√©tat de l'API et du mod√®le
    
    Returns:
        HealthResponse avec l'√©tat du syst√®me
    """
    try:
        service = get_prediction_service()
        
        model_loaded = service.model is not None
        vectorizer_loaded = service.vectorizer is not None
        
        if model_loaded and vectorizer_loaded:
            return HealthResponse(
                status="healthy",
                model_loaded=True,
                vectorizer_loaded=True,
                message="API et mod√®le op√©rationnels"
            )
        else:
            return HealthResponse(
                status="unhealthy",
                model_loaded=model_loaded,
                vectorizer_loaded=vectorizer_loaded,
                message="Mod√®le ou vectoriseur non charg√©"
            )
    
    except Exception as e:
        logger.error(f"Erreur health check: {e}")
        return HealthResponse(
            status="error",
            model_loaded=False,
            vectorizer_loaded=False,
            message=f"Erreur: {str(e)}"
        )

@app.post("/predict_batch", response_model=BatchPredictionResponse, tags=["Prediction"])
async def predict_batch(batch: CommentBatch):
    """
    Analyse le sentiment d'un batch de commentaires
    
    Args:
        batch: Batch de commentaires √† analyser
        
    Returns:
        BatchPredictionResponse avec pr√©dictions et statistiques
    """
    start_time = time.time()
    
    try:
        # R√©cup√©rer le service
        service = get_prediction_service()
        
        if not service.is_loaded():
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="Service de pr√©diction non disponible"
            )
        
        # Valider le nombre de commentaires
        if len(batch.comments) > 100:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Maximum 100 commentaires par batch"
            )
        
        logger.info(f"üì• R√©ception de {len(batch.comments)} commentaires")
        
        # Faire les pr√©dictions
        predictions = service.predict_batch(batch.comments)
        
        # Calculer les statistiques
        statistics = service.calculate_statistics(predictions)
        
        # Temps de traitement
        processing_time = (time.time() - start_time) * 1000  # en ms
        
        logger.info(f"‚úÖ Traitement termin√© en {processing_time:.2f}ms")
        
        # Convertir en mod√®les Pydantic
        prediction_models = [
            SentimentPrediction(**pred) for pred in predictions
        ]
        
        return BatchPredictionResponse(
            predictions=prediction_models,
            statistics=statistics,
            total_comments=len(predictions),
            processing_time_ms=round(processing_time, 2)
        )
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la pr√©diction: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Erreur interne: {str(e)}"
        )

@app.post("/predict", response_model=SentimentPrediction, tags=["Prediction"])
async def predict_single(comment: str):
    """
    Analyse le sentiment d'un seul commentaire
    
    Args:
        comment: Commentaire √† analyser
        
    Returns:
        SentimentPrediction avec le r√©sultat
    """
    try:
        service = get_prediction_service()
        
        if not service.is_loaded():
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="Service de pr√©diction non disponible"
            )
        
        if not comment or not comment.strip():
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Le commentaire ne peut pas √™tre vide"
            )
        
        prediction = service.predict_single(comment.strip())
        
        return SentimentPrediction(**prediction)
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la pr√©diction: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Erreur interne: {str(e)}"
        )

# Gestionnaire d'erreurs global
@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    """Gestionnaire global des exceptions"""
    logger.error(f"Exception non g√©r√©e: {exc}")
    return JSONResponse(
        status_code=500,
        content={
            "error": "Internal Server Error",
            "detail": str(exc)
        }
    )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "src.api.main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )